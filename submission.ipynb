{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import io\n",
    "import librosa\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt    \n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "import glob\n",
    "import pandas as pd \n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "import torch\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt    \n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./kaggle/input/birdclef-2023/test_soundscapes/soundscape_29201.ogg']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# test_samples = list(os.listdir(\"./kaggle/input/birdclef-2023/test_soundscapes/*.ogg\"))\n",
    "test_samples = list(glob.glob(\"./kaggle/input/birdclef-2023/test_soundscapes/*.ogg\"))\n",
    "test_samples\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAudioDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.test_samples = list(glob.glob(\"./kaggle/input/birdclef-2023/test_soundscapes/*.ogg\"))\n",
    "         \n",
    "    def __len__(self):\n",
    "        return len(self.test_samples)\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        path = self.test_samples[idx]\n",
    "        file_id = path.split(\".ogg\")[0].split(\"/\")[-1]\n",
    "        \n",
    "        waveform, sample_rate = torchaudio.load(path)\n",
    "        return ((waveform[0], sample_rate), file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stride_trick(a, stride_length, stride_step):\n",
    "     \"\"\"\n",
    "     apply framing using the stride trick from numpy.\n",
    "\n",
    "     Args:\n",
    "         a (array) : signal array.\n",
    "         stride_length (int) : length of the stride.\n",
    "         stride_step (int) : stride step.\n",
    "\n",
    "     Returns:\n",
    "         blocked/framed array.\n",
    "     \"\"\"\n",
    "     nrows = ((a.size - stride_length) // stride_step) + 1\n",
    "     n = a.strides[0]\n",
    "     return np.lib.stride_tricks.as_strided(a,\n",
    "                                            shape=(nrows, stride_length),\n",
    "                                            strides=(stride_step*n, n))\n",
    "\n",
    "def framing(sig, fs=16000, win_len=0.025, win_hop=0.01):\n",
    "     \"\"\"\n",
    "     transform a signal into a series of overlapping frames (=Frame blocking).\n",
    "\n",
    "     Args:\n",
    "         sig     (array) : a mono audio signal (Nx1) from which to compute features.\n",
    "         fs        (int) : the sampling frequency of the signal we are working with.\n",
    "                           Default is 16000.\n",
    "         win_len (float) : window length in sec.\n",
    "                           Default is 0.025.\n",
    "         win_hop (float) : step between successive windows in sec.\n",
    "                           Default is 0.01.\n",
    "\n",
    "     Returns:\n",
    "         array of frames.\n",
    "         frame length.\n",
    "\n",
    "     Notes:\n",
    "     ------\n",
    "         Uses the stride trick to accelerate the processing.\n",
    "     \"\"\"\n",
    "     # run checks and assertions\n",
    "     if win_len < win_hop: print(\"ParameterError: win_len must be larger than win_hop.\")\n",
    "\n",
    "     # compute frame length and frame step (convert from seconds to samples)\n",
    "     frame_length = win_len * fs\n",
    "     frame_step = win_hop * fs\n",
    "     signal_length = len(sig)\n",
    "     frames_overlap = frame_length - frame_step\n",
    "\n",
    "     # compute number of frames and left sample in order to pad if needed to make\n",
    "     # sure all frames have equal number of samples  without truncating any samples\n",
    "     # from the original signal\n",
    "     rest_samples = np.abs(signal_length - frames_overlap) % np.abs(frame_length - frames_overlap)\n",
    "     pad_signal = np.append(sig, np.array([0] * int(frame_step - rest_samples) * int(rest_samples != 0.)))\n",
    "\n",
    "     # apply stride trick\n",
    "     frames = stride_trick(pad_signal, int(frame_length), int(frame_step))\n",
    "     return frames, frame_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessing: \n",
    "    @staticmethod\n",
    "    def record_to_frames(waveform, sample_rate, frame_size=5):\n",
    "        p1d = (1, sample_rate * frame_size)\n",
    "        out = torch.nn.functional.pad(waveform, p1d, \"constant\", 0)\n",
    "        return out.unfold(0, sample_rate * frame_size, sample_rate * frame_size)\n",
    "\n",
    "    @staticmethod\n",
    "    def my_collate(batch):\n",
    "        frames = []\n",
    "        labels = []\n",
    "        for (data,file_id) in batch: \n",
    "                (waveform, sample_rate) = data\n",
    "                l_frames, frame_length = framing(waveform, sample_rate, 5.0, 5.0)\n",
    "   \n",
    "                for index in range(len(l_frames)):\n",
    "                    frame = l_frames[index]\n",
    "                    # audio_spectogram = spectogram(frame)\n",
    "                    # audio_spectogram = audio_spectogram.repeat(3, 1, 1)\n",
    "                    frames.append((frame, sample_rate, f'{file_id}_{(index+1)*5}'))\n",
    "                    labels.append(file_id) \n",
    "        return [frames, labels]\n",
    "    \n",
    "    @staticmethod\n",
    "    def melgram_v2(audio, sample_rate,  to_file):\n",
    "         \n",
    "        plt.figure(figsize=(3,2))\n",
    "        plt.axis('off')  # no axis\n",
    "        plt.axes([0., 0., 1., 1.], frameon=False, xticks=[], yticks=[])  # Remove the white edge\n",
    "        melspectrogram = librosa.feature.melspectrogram(y=audio, sr=sample_rate)\n",
    "       \n",
    "        # height = S.shape[0]  \n",
    "        # image_cropped = S[int(height*0.2  ):int(height*0.8 ),:]\n",
    "        librosa.display.specshow(librosa.power_to_db(melspectrogram, ref=np.max))\n",
    "        plt.savefig(to_file, bbox_inches=None, pad_inches=0)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSet = TrainAudioDataset()\n",
    "batch_size = 1\n",
    "dataLoader = DataLoader(dataSet, batch_size=batch_size, collate_fn=DataProcessing.my_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(dataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (X, y) = next(it)\n",
    "# print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120it [00:13,  8.98it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for (X, Y) in dataLoader:\n",
    "    for batch, x in tqdm(enumerate(X)):\n",
    "        fileName = Y[batch]\n",
    "        (frame, sample_rate, name) = x\n",
    "        path =  f'./kaggle/input/birdclef-2023/test_melspectrogram/{fileName}/'\n",
    "        Path(path).mkdir(parents=True, exist_ok=True)\n",
    "        DataProcessing.melgram_v2(frame , sample_rate, f'{path}/{name}.png')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([ transforms.ToTensor() ])\n",
    "dataset = datasets.ImageFolder('./kaggle/input/birdclef-2023/test_melspectrogram', transform=transform,  )\n",
    "testloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lex/miniconda3/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/lex/miniconda3/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=264, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = sorted(os.listdir('./kaggle/input/birdclef-2023/train_audio'))\n",
    "\n",
    "model_ft = models.resnet18(pretrained=False)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = torch.nn.Linear(num_ftrs, len(class_names))\n",
    "model_ft = model_ft.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = torch.optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "checkpoint = torch.load('./models/model2023-04-028.pth', map_location=torch.device('cpu'))\n",
    "model_ft.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer_ft.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "model_ft.eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>abethr1</th>\n",
       "      <th>abhori1</th>\n",
       "      <th>abythr1</th>\n",
       "      <th>afbfly1</th>\n",
       "      <th>afdfly1</th>\n",
       "      <th>afecuc1</th>\n",
       "      <th>affeag1</th>\n",
       "      <th>afgfly1</th>\n",
       "      <th>afghor1</th>\n",
       "      <th>...</th>\n",
       "      <th>yebsto1</th>\n",
       "      <th>yeccan1</th>\n",
       "      <th>yefcan</th>\n",
       "      <th>yelbis1</th>\n",
       "      <th>yenspu1</th>\n",
       "      <th>yertin1</th>\n",
       "      <th>yesbar1</th>\n",
       "      <th>yespet1</th>\n",
       "      <th>yetgre1</th>\n",
       "      <th>yewgre1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 265 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [row_id, abethr1, abhori1, abythr1, afbfly1, afdfly1, afecuc1, affeag1, afgfly1, afghor1, afmdov1, afpfly1, afpkin1, afpwag1, afrgos1, afrgrp1, afrjac1, afrthr1, amesun2, augbuz1, bagwea1, barswa, bawhor2, bawman1, bcbeat1, beasun2, bkctch1, bkfruw1, blacra1, blacuc1, blakit1, blaplo1, blbpuf2, blcapa2, blfbus1, blhgon1, blhher1, blksaw1, blnmou1, blnwea1, bltapa1, bltbar1, bltori1, blwlap1, brcale1, brcsta1, brctch1, brcwea1, brican1, brobab1, broman1, brosun1, brrwhe3, brtcha1, brubru1, brwwar1, bswdov1, btweye2, bubwar2, butapa1, cabgre1, carcha1, carwoo1, categr, ccbeat1, chespa1, chewea1, chibat1, chtapa3, chucis1, cibwar1, cohmar1, colsun2, combul2, combuz1, comsan, crefra2, crheag1, crohor1, darbar1, darter3, didcuc1, dotbar1, dutdov1, easmog1, eaywag1, edcsun3, egygoo, equaka1, eswdov1, eubeat1, fatrav1, fatwid1, fislov1, fotdro5, gabgos2, gargan, gbesta1, gnbcam2, gnhsun1, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 265 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sub:pd.DataFrame = pd.read_csv(\"./kaggle/input/birdclef-2023/sample_submission.csv\")\n",
    "sample_sub[class_names] = sample_sub[class_names].astype(np.float32)\n",
    "sample_sub.drop(sample_sub.index, inplace=True)\n",
    "sample_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(89)\n",
      "tensor(221)\n",
      "tensor(8)\n",
      "tensor(229)\n",
      "tensor(221)\n",
      "tensor(8)\n",
      "tensor(221)\n",
      "tensor(8)\n",
      "tensor(8)\n",
      "tensor(8)\n",
      "tensor(229)\n",
      "tensor(132)\n",
      "tensor(169)\n",
      "tensor(230)\n",
      "tensor(31)\n",
      "tensor(30)\n",
      "tensor(25)\n",
      "tensor(168)\n",
      "tensor(168)\n",
      "tensor(25)\n",
      "tensor(247)\n",
      "tensor(221)\n",
      "tensor(20)\n",
      "tensor(221)\n",
      "tensor(221)\n",
      "tensor(165)\n",
      "tensor(221)\n",
      "tensor(247)\n",
      "tensor(221)\n",
      "tensor(233)\n",
      "tensor(163)\n",
      "tensor(70)\n",
      "tensor(70)\n",
      "tensor(56)\n",
      "tensor(70)\n",
      "tensor(137)\n",
      "tensor(47)\n",
      "tensor(47)\n",
      "tensor(132)\n",
      "tensor(221)\n",
      "tensor(221)\n",
      "tensor(20)\n",
      "tensor(20)\n",
      "tensor(20)\n",
      "tensor(165)\n",
      "tensor(20)\n",
      "tensor(20)\n",
      "tensor(20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(20)\n",
      "tensor(224)\n",
      "tensor(225)\n",
      "tensor(73)\n",
      "tensor(221)\n",
      "tensor(20)\n",
      "tensor(20)\n",
      "tensor(165)\n",
      "tensor(20)\n",
      "tensor(131)\n",
      "tensor(73)\n",
      "tensor(73)\n",
      "tensor(73)\n",
      "tensor(165)\n",
      "tensor(73)\n",
      "tensor(56)\n",
      "tensor(17)\n",
      "tensor(20)\n",
      "tensor(132)\n",
      "tensor(20)\n",
      "tensor(135)\n",
      "tensor(73)\n",
      "tensor(73)\n",
      "tensor(163)\n",
      "tensor(73)\n",
      "tensor(20)\n",
      "tensor(165)\n",
      "tensor(73)\n",
      "tensor(221)\n",
      "tensor(53)\n",
      "tensor(225)\n",
      "tensor(132)\n",
      "tensor(20)\n",
      "tensor(245)\n",
      "tensor(20)\n",
      "tensor(10)\n",
      "tensor(214)\n",
      "tensor(132)\n",
      "tensor(221)\n",
      "tensor(165)\n",
      "tensor(93)\n",
      "tensor(113)\n",
      "tensor(20)\n",
      "tensor(73)\n",
      "tensor(84)\n",
      "tensor(221)\n",
      "tensor(221)\n",
      "tensor(132)\n",
      "tensor(165)\n",
      "tensor(73)\n",
      "tensor(20)\n",
      "tensor(20)\n",
      "tensor(10)\n",
      "tensor(131)\n",
      "tensor(132)\n",
      "tensor(27)\n",
      "tensor(20)\n",
      "tensor(20)\n",
      "tensor(131)\n",
      "tensor(221)\n",
      "tensor(20)\n",
      "tensor(20)\n",
      "tensor(221)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:02,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8)\n",
      "tensor(128)\n",
      "tensor(132)\n",
      "tensor(132)\n",
      "tensor(113)\n",
      "tensor(6)\n",
      "tensor(131)\n",
      "tensor(221)\n",
      "tensor(221)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "indexToClass = {v: k for k, v in dataset.class_to_idx.items()}\n",
    "columns = []\n",
    " \n",
    "with torch.no_grad():\n",
    "    for batch, (X, y) in tqdm(enumerate(testloader)):\n",
    "        pred = model_ft(X)\n",
    "        for index, predItem in enumerate(pred):\n",
    "            m = torch.nn.Softmax(dim=0)\n",
    "            output = m(predItem) \n",
    "            print(output.argmax(0))\n",
    "            fileName = dataset.imgs[batch * 64 + index][0].split('/')[-1].split('.')[0]\n",
    "             \n",
    "            columns = sample_sub.columns\n",
    "            \n",
    "            collection = np.append([fileName], output.detach().numpy())\n",
    "    \n",
    "            newRow = pd.DataFrame([collection], columns=sample_sub.columns)\n",
    "            pd.DataFrame(newRow)\n",
    "            sample_sub = pd.concat([sample_sub, newRow], ignore_index=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'soundscape_29201_10': tensor(89), 'soundscape_29201_100': tensor(221), 'soundscape_29201_105': tensor(8), 'soundscape_29201_110': tensor(229), 'soundscape_29201_115': tensor(221), 'soundscape_29201_120': tensor(8), 'soundscape_29201_125': tensor(221), 'soundscape_29201_130': tensor(8), 'soundscape_29201_135': tensor(8), 'soundscape_29201_140': tensor(8), 'soundscape_29201_145': tensor(229), 'soundscape_29201_15': tensor(132), 'soundscape_29201_150': tensor(169), 'soundscape_29201_155': tensor(230), 'soundscape_29201_160': tensor(31), 'soundscape_29201_165': tensor(30), 'soundscape_29201_170': tensor(25), 'soundscape_29201_175': tensor(168), 'soundscape_29201_180': tensor(168), 'soundscape_29201_185': tensor(25), 'soundscape_29201_190': tensor(247), 'soundscape_29201_195': tensor(221), 'soundscape_29201_20': tensor(20), 'soundscape_29201_200': tensor(221), 'soundscape_29201_205': tensor(221), 'soundscape_29201_210': tensor(165), 'soundscape_29201_215': tensor(221), 'soundscape_29201_220': tensor(247), 'soundscape_29201_225': tensor(221), 'soundscape_29201_230': tensor(233), 'soundscape_29201_235': tensor(163), 'soundscape_29201_240': tensor(70), 'soundscape_29201_245': tensor(70), 'soundscape_29201_25': tensor(56), 'soundscape_29201_250': tensor(70), 'soundscape_29201_255': tensor(137), 'soundscape_29201_260': tensor(47), 'soundscape_29201_265': tensor(47), 'soundscape_29201_270': tensor(132), 'soundscape_29201_275': tensor(221), 'soundscape_29201_280': tensor(221), 'soundscape_29201_285': tensor(20), 'soundscape_29201_290': tensor(20), 'soundscape_29201_295': tensor(20), 'soundscape_29201_30': tensor(165), 'soundscape_29201_300': tensor(20), 'soundscape_29201_305': tensor(20), 'soundscape_29201_310': tensor(20), 'soundscape_29201_315': tensor(20), 'soundscape_29201_320': tensor(224), 'soundscape_29201_325': tensor(225), 'soundscape_29201_330': tensor(73), 'soundscape_29201_335': tensor(221), 'soundscape_29201_340': tensor(20), 'soundscape_29201_345': tensor(20), 'soundscape_29201_35': tensor(165), 'soundscape_29201_350': tensor(20), 'soundscape_29201_355': tensor(131), 'soundscape_29201_360': tensor(73), 'soundscape_29201_365': tensor(73), 'soundscape_29201_370': tensor(73), 'soundscape_29201_375': tensor(165), 'soundscape_29201_380': tensor(73), 'soundscape_29201_385': tensor(56), 'soundscape_29201_390': tensor(17), 'soundscape_29201_395': tensor(20), 'soundscape_29201_40': tensor(132), 'soundscape_29201_400': tensor(20), 'soundscape_29201_405': tensor(135), 'soundscape_29201_410': tensor(73), 'soundscape_29201_415': tensor(73), 'soundscape_29201_420': tensor(163), 'soundscape_29201_425': tensor(73), 'soundscape_29201_430': tensor(20), 'soundscape_29201_435': tensor(165), 'soundscape_29201_440': tensor(73), 'soundscape_29201_445': tensor(221), 'soundscape_29201_45': tensor(53), 'soundscape_29201_450': tensor(225), 'soundscape_29201_455': tensor(132), 'soundscape_29201_460': tensor(20), 'soundscape_29201_465': tensor(245), 'soundscape_29201_470': tensor(20), 'soundscape_29201_475': tensor(10), 'soundscape_29201_480': tensor(214), 'soundscape_29201_485': tensor(132), 'soundscape_29201_490': tensor(221), 'soundscape_29201_495': tensor(165), 'soundscape_29201_5': tensor(93), 'soundscape_29201_50': tensor(113), 'soundscape_29201_500': tensor(20), 'soundscape_29201_505': tensor(73), 'soundscape_29201_510': tensor(84), 'soundscape_29201_515': tensor(221), 'soundscape_29201_520': tensor(221), 'soundscape_29201_525': tensor(132), 'soundscape_29201_530': tensor(165), 'soundscape_29201_535': tensor(73), 'soundscape_29201_540': tensor(20), 'soundscape_29201_545': tensor(20), 'soundscape_29201_55': tensor(10), 'soundscape_29201_550': tensor(131), 'soundscape_29201_555': tensor(132), 'soundscape_29201_560': tensor(27), 'soundscape_29201_565': tensor(20), 'soundscape_29201_570': tensor(20), 'soundscape_29201_575': tensor(131), 'soundscape_29201_580': tensor(221), 'soundscape_29201_585': tensor(20), 'soundscape_29201_590': tensor(20), 'soundscape_29201_595': tensor(221), 'soundscape_29201_60': tensor(8), 'soundscape_29201_600': tensor(128), 'soundscape_29201_65': tensor(132), 'soundscape_29201_70': tensor(132), 'soundscape_29201_75': tensor(113), 'soundscape_29201_80': tensor(6), 'soundscape_29201_85': tensor(131), 'soundscape_29201_90': tensor(221), 'soundscape_29201_95': tensor(221)}\n"
     ]
    }
   ],
   "source": [
    "print(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6268/1559306738.py:1: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  torch.FloatTensor(sample_sub.loc[9][1:].values.astype(np.float)).argmax(0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(8)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.FloatTensor(sample_sub.loc[9][1:].values.astype(np.float)).argmax(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'afgfly1'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sub.columns[8]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b875fe5df27458d6e5386338453ec9dcfa4b2929984d5f2a45aefcdea2b819bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
